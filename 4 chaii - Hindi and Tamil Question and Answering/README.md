<img width="954" alt="스크린샷 2021-11-16 오후 12 05 27" src="https://user-images.githubusercontent.com/58493928/141985824-129f3a85-be30-4fd2-85bb-570b2dce3469.png">


```bash
|-- README.md
|-- Ranked 73/943 (Top 8%) - Bronze medal is awarded.
# |-- TabNet.md
# |-- Realized Volitility(ref1).md
```

# [chaii - Hindi and Tamil Question and Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering)
- 2021.Aug ~ 2021.Nov (completed)
  - 73/943 (Top 8% - Bronze medal)
- Host
  - Google: Google Research India
- Goal
  -  Predict answers to real questions about Wikipedia articles by using chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. 

  - With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users. With more attention from the Kaggle community and your novel machine learning solutions, we can help Indian users make the most of the web.
  - Predicting answers to questions is a common NLU task, but not for Hindi and Tamil. Current progress on multilingual modeling requires a concentrated effort to generate high-quality datasets and modelling improvements. Additionally, for languages that are typically underrepresented in public datasets, it can be difficult to build trustworthy evaluations. We hope the dataset provided for this competition — and additional datasets generated by participants — will enable future machine learning for Indian languages. 

- Evaluation
  - The metric in this competition is the word-level Jaccard score.
  	```python
  	def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
  	```   
  - The formula for the overal metric is:
  	> score = $\frac{1}{n}\sum_{i=1}^{n} jaccard(gt_i, dt_i)$
  	where n = number of documents, jaccard = the function provided above, $gt_i$ = the ith ground truth, $dt_i$ = the ith prediction  

  - Submission File: For each ID in the test set, you must predict the string that best answers the provided question based on the context. Note that the selected text needs to be quoted and complete to work correctly. Include punctuation, etc. - the above code splits ONLY on whitespace. The file should contain a header and have the following format:
    ```
    id,PredictionString
		8c8ee6504,"1"
		3163c22d0,"2 string"
		66aae423b,"4 word 6"
		722085a7b,"1"
		etc.
    ```
- Prizes
    ```
    1st Place - $2,000
		2nd Place - $2,000
		3rd Place - $2,000
		4th Place - $2,000
		5th Place - $2,000
    ```
- Code Requirements
  - Submissions to this competition must be made through Notebooks. In order for the "Submit" button to be active after a commit, the following conditions must be met:
    ```
    CPU Notebook <= 5 hours run-time
    GPU Notebook <= 5 hours run-time
    Internet access disabled
    Freely & publicly available external data is allowed, including pre-trained models
    Submission file must be named submission.csv
    ```
- Data
  - In this competition, you will be predicting the answers to questions in Hindi and Tamil. The answers are drawn directly (see the Evaluation page for details) from a limited context. We have provided a small number of samples to check your code with. There is also a hidden test set. All files should be encoded as UTF-8.

  - Files		
		* `train.csv` - the training set, containing context, questions, and answers. Also includes the start character of the answer for disambiguation.
		* `test.csv` - the test set, containing context and questions.
		* `sample_submission.csv` - a sample submission file in the correct format
		
	- Columns
    * `id` - a unique identifier.
		* `context` - the text of the Hindi/Tamil sample from which answers should be derived
		* `question` - the question, in Hindi/Tamil.
		* `answer_test (train only)` -  the answer to the question (manual annotation) (note: for test, this is what you are attempting to predict)
		* `answer_start (train only)` - the starting character in context for the answer (determined using substring match during data preparation)
		* `language` - whether the text in question is in Tamil or Hindi
	
	- Data Annotation Details
		* chaii 2021 dataset was prepared following the two step process as in TydiQA.
		* In the question elicitation step, the annotators were shown snippets of Wikipedia text and asked to come up with interesting questions that they may be genuinely interested in knowing answers about. They were also asked to make sure the elicited question was not answerable from the snippet of wiki text shown. Annotators were asked to elicit questions which were likely to have precise, unambiguous answers.
		* In the answer labelling step, for each question elicited in the previous step, the first Wikipedia page in the Google search results for that question was selected. For Hindi questions, the selection was restricted to Hindi Wikipedia documents, and similarly for Tamil. Annotators were then asked to select the answer for the question in the document. Annotators were asked to select the first valid answer in the document as the correct answer.
		* Questions which were not answerable from the selected document were marked as non-answerable. These question-document pairs were not included in the chaii 2021 dataset.
		* With (question, wiki_document, answer) now in place, the first substring occurrence of the answer in the wiki_document was automatically calculated and provided as answer_start in the dataset. Since this part was done automatically, some amount of inaccuracy is possible. This was included only for convenience, and participants may consider ignoring this offset during model development (or come up with their own mechanism for offset selection). Please note that during test, the model is only required to predict the answer string, and not its span offset.
		* Answers in the training data were produced by one annotator, while those in the test were produced by three annotators. Majority voting was then used to come up with the final answer. In test data with minor disagreements, a separate annotator pass was done to select the final answer. For both train and test answer labelling, sampling based quality checks were carried out and the answer accuracy were routinely observed to be quite high.
		* In spite of all these multi-step checks, some amount of noise in the training data is likely. This is expected and meant to reflect real-world settings where slight noise in the training data may be unavoidable to achieve larger volumes of it. Moreover, this may also result in development of more robust methods which are more noise tolerant during training.
		* Update: we ran a few random sampling based quality checks on the datasets. Based on these checks, we found the Hindi train and test datasets to be 93.8% and 97.8%, respectively. No issues were identified in the sampled Tamil train and test instances.
  - `kaggle competitions download -c chaii-hindi-and-tamil-question-answering`.
